{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262e14e1",
   "metadata": {},
   "source": [
    "##  estimate phenology from gimms ndvi\n",
    "\n",
    "1. process raw ndvi data: choose site covered by plant, replace negative ndvi, savgol-filter\n",
    "\n",
    "2. fit model to get daily ndvi sequence\n",
    "\n",
    "3. get phenology using threshold or change point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36053e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47459531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import savitzky_golay\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import scipy.interpolate as spi\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ndvi_data(paths):\n",
    "    ndvi = []\n",
    "    for file in paths:\n",
    "        print(file)\n",
    "        \n",
    "        with xr.open_dataset(file,chunks=({'lat':1000,'lon':100})) as ndvi_i:\n",
    "            ndvi.append(ndvi_i['ndvi'])\n",
    "        \n",
    "    ndvi_data = xr.concat(ndvi, dim = 'time')\n",
    "    ndvi_data = ndvi_data.sortby('time')\n",
    "        \n",
    "    ndvi_data = ndvi_data.chunk({'lat':1000, 'lon':100, 'time':-1})\n",
    "        \n",
    "    return ndvi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eeaa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbl_logistic_model(p, doys):\n",
    "    y = p[0] + p[1] * (1./(1+np.exp(p[2]*(doys-p[3]))) + \n",
    "                      1./(1 + np.exp(-p[4]*(doys - p[5]))))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8599eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mismatch_function(p, pheno_func, data, days):\n",
    "    output = []\n",
    "    \n",
    "    fitness = lambda p, data_in, days: data_in - pheno_func(p, days)\n",
    "    \n",
    "    oot = fitness( p , data, days)\n",
    "    [output.append(x) for x in oot]\n",
    "    \n",
    "    return np.array(output).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_double_logistic(data, t_axis, xinit = None):\n",
    "    \n",
    "    from scipy.optimize import leastsq\n",
    "    \n",
    "    pheno_func = dbl_logistic_model\n",
    "    \n",
    "    n_params = 6\n",
    "    if xinit is None:\n",
    "        xinit = [.5,] * n_params\n",
    "        xinit[0] = data.min()\n",
    "        xinit[1] = data.max() - data.min()\n",
    "        xinit[2] = 0.19\n",
    "        xinit[3] = 120\n",
    "        xinit[4] = 0.13\n",
    "        xinit[5] = 260\n",
    "        \n",
    "    (xsol , msg) = leastsq(mismatch_function, xinit, maxfev=1000000,\n",
    "                           args = (pheno_func, data ,t_axis))\n",
    "    \n",
    "    ax = pheno_func( xsol, np.arange(1,366))\n",
    "    \n",
    "    return (xsol, msg, ax)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piece_logistic(p,doys):\n",
    "    y = p[0] + (p[1] - p[0])/(1+np.exp(p[2]*(p[3]-doys)))\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_piece_logistic( data, t_axis, xinit = None, season = 'sos'):\n",
    "    from scipy.optimize import leastsq\n",
    "    \n",
    "    pheno_func = piece_logistic\n",
    "    n_params = 4\n",
    "    \n",
    "    if xinit is None:\n",
    "        xinit = [.5] * n_params\n",
    "        \n",
    "        xinit[0] = data.min()\n",
    "        xinit[1] = data.max()\n",
    "        xinit[2] = 0.15\n",
    "        \n",
    "        if season == 'sos':\n",
    "            xinit[3] = 120\n",
    "        else:\n",
    "            xinit[3] = 250\n",
    "        \n",
    "    (xsol, msg) = leastsq( mismatch_function,xinit, maxfev= 1000000,\n",
    "                         args = (pheno_func, data, t_axis))\n",
    "    \n",
    "    if season == 'sos':\n",
    "        ax = pheno_func(xsol, np.arange(1,t_axis.max()))\n",
    "    else:\n",
    "        ax = pheno_func(xsol, np.arange(t_axis.min(),t_axis.max()))\n",
    "    return (xsol, msg, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndvi_phenology(ndvi_data, ndvi_mean):\n",
    "    \n",
    "    phenos = np.full(8, np.nan)\n",
    "    \n",
    "    ## check data of points\n",
    "    if np.sum(np.isnan(ndvi_data)) < 12 and ndvi_mean > 0.1 :\n",
    "        \n",
    "        #doy = np.arange(8,365,15)  ## gimms ndvi\n",
    "        doy = np.arange(8,365,16)  ## modis 16d ndvi\n",
    "        ndvi_data = np.nan_to_num(ndvi_data, nan = ndvi_mean)\n",
    "        growing_season = np.mean(ndvi_data[10:16])\n",
    "        non_growing_1 = np.mean(ndvi_data[np.arange(6)])\n",
    "        non_growing_2 = np.mean(ndvi_data[np.arange(20,len(ndvi_data))])\n",
    "        non_growing_season = np.mean([non_growing_1,non_growing_2])\n",
    "        \n",
    "        doy_mid_raw = doy[ndvi_data.argmax()]\n",
    "        is_mid = doy_mid_raw > 150 and doy_mid_raw < 270\n",
    "        \n",
    "        if growing_season > (1.2*non_growing_season)  and is_mid:\n",
    "            \n",
    "            ## SG-filter\n",
    "            ndvi_data = np.asarray(ndvi_data[:len(doy)])\n",
    "            ndvi_data_sg = savitzky_golay.savitzky_golay(ndvi_data,15,4)\n",
    "            \n",
    "            #plt.plot(ndvi_data_sg)\n",
    "            ## divide into spr_data and aut_data\n",
    "            index_mid = ndvi_data_sg.argmax()\n",
    "            doy_mid = doy[index_mid]\n",
    "            \n",
    "            if doy_mid >160 and doy_mid < 220: \n",
    "                doy_spr = doy[:index_mid]\n",
    "                doy_aut = doy[index_mid:]\n",
    "\n",
    "                data_sg_spr = ndvi_data_sg[:index_mid]\n",
    "                data_sg_aut = ndvi_data_sg[index_mid:]\n",
    "            \n",
    "            else:\n",
    "                doy_spr = doy[doy <= 200]\n",
    "                doy_aut = doy[doy > 170]\n",
    "            \n",
    "                data_sg_spr = ndvi_data_sg[doy <= 200]\n",
    "                data_sg_aut = ndvi_data_sg[doy > 170]\n",
    "            \n",
    "            ## method 1: poly 50%\n",
    "            poly_model = poly.polyfit(doy,ndvi_data_sg,6)\n",
    "            poly_ndvi = poly.polyval(np.arange(1,366),poly_model)\n",
    "            \n",
    "            poly_ratio = (poly_ndvi- poly_ndvi.min()) / (poly_ndvi.max() - \n",
    "                                              poly_ndvi.min())\n",
    "            \n",
    "            if len(poly_ratio) > 0:\n",
    "                phenos[0] = np.asarray(poly_ratio > 0.5).nonzero()[0][0]\n",
    "                phenos[4] = np.asarray(poly_ratio > 0.5).nonzero()[0][-1]    \n",
    "            \n",
    "            ## method 2: spline 50%\n",
    "            spi_spline = spi.splrep(doy,ndvi_data_sg,k=3)\n",
    "            result_spline_3 = spi.splev(np.arange(1,366),spi_spline)\n",
    "\n",
    "            ratio_spline = (result_spline_3 - result_spline_3.min()) / (\n",
    "                result_spline_3.max() - result_spline_3.min())\n",
    "            \n",
    "            if len(np.asarray(ratio_spline > 0.5).nonzero()[0]) > 0:\n",
    "                phenos[1] = np.asarray(ratio_spline > 0.5).nonzero()[0][0]\n",
    "                phenos[5] = np.asarray(ratio_spline > 0.5).nonzero()[0][-1]\n",
    "    \n",
    "            ## method 3: dlog\n",
    "            dlog_model = fit_double_logistic(ndvi_data_sg,doy)    \n",
    "            \n",
    "            dbl_fitted_ndvi = dlog_model[2]\n",
    "            \n",
    "            if len(dbl_fitted_ndvi) > 0:\n",
    "                dbl_ndvi_1der = np.diff(dbl_fitted_ndvi)\n",
    "            \n",
    "                phenos[2] = np.arange(1,365)[dbl_ndvi_1der.argmax()]\n",
    "                phenos[6] = np.arange(1,365)[dbl_ndvi_1der.argmin()]\n",
    "    \n",
    "            ## method 4: plog\n",
    "            plog_spr = fit_piece_logistic(data_sg_spr, doy_spr)\n",
    "            plog_aut = fit_piece_logistic(data_sg_aut, doy_aut, season = 'aut')\n",
    "            \n",
    "            if len(plog_spr[2]) > 0:\n",
    "                plog_fitted_ndvi_spr = plog_spr[2]\n",
    "                plog_spr_1der = np.diff(plog_fitted_ndvi_spr)\n",
    "                \n",
    "                phenos[3] = np.arange(1,doy_spr.max())[plog_spr_1der.argmax()]\n",
    "                \n",
    "            if len(plog_aut[2]) > 0:\n",
    "                plog_fitted_ndvi_aut = plog_aut[2]\n",
    "                plog_aut_1der = np.diff(plog_fitted_ndvi_aut)\n",
    "            \n",
    "                phenos[7] = np.arange(doy_aut.min(),366)[plog_aut_1der.argmin()]\n",
    "\n",
    "    \n",
    "    return phenos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac920879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndvi_pheno(x,ndvi_mean):\n",
    "    return xr.apply_ufunc(\n",
    "        get_ndvi_phenology,\n",
    "        x,\n",
    "        ndvi_mean,\n",
    "        input_core_dims = [['time'],[]],\n",
    "        output_core_dims = [['pheno']],\n",
    "        dask_gufunc_kwargs = {'output_sizes':{'pheno':8}},\n",
    "        vectorize = True,\n",
    "        dask = 'parallelized',\n",
    "        output_dtypes= [float]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb041db",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2001\n",
    "end_year = 2020\n",
    "lat_lon = '-75-70'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_path = glob.glob(r'D:/aithc/phd/data/modis_ndvi/modis_preprocessed/*'+lat_lon+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8160d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndvi_data = read_ndvi_data(ndvi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_mean = ndvi_data.mean(dim = 'time', skipna =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad24d7",
   "metadata": {},
   "source": [
    "# test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e15597",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_year = ndvi_data.groupby('time.year')[2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f8e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_data = ndvi_year[100,100,:].values\n",
    "point_data\n",
    "\n",
    "get_ndvi_phenology(point_data, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31184b2",
   "metadata": {},
   "source": [
    "# get phenology by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8af3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year+1):\n",
    "    \n",
    "    print('now calculting....',year)\n",
    "    \n",
    "    ndvi_year = ndvi_data.groupby('time.year')[year]\n",
    "    \n",
    "    result = ndvi_pheno(ndvi_year, ndvi_mean)\n",
    "    \n",
    "    with ProgressBar():\n",
    "        result_phenos = result.compute()\n",
    "        \n",
    "    pheno_list = year*100 + np.asarray([11,12,13,14,21,22,23,24])\n",
    "    \n",
    "    result_phenos['pheno'] = pheno_list\n",
    "    result_phenos.name = 'phenology'\n",
    "    \n",
    "    result_phenos.to_netcdf(path = 'D:/aithc/phd/data/modis_ndvi/pheno/'+str(year)+lat_lon+'.nc',\n",
    "                           encoding = {'phenology':{'dtype':'int16','_FillValue':-9999,\n",
    "                                                   'zlib':True,'complevel':9}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4e4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0551d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
